{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f33984ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "IsADirectoryError",
     "evalue": "[Errno 21] Is a directory: '/Users/christianmulder/Desktop/CASA/FSDS_Groupwork/01_Data/Raw/20250615-London-listings.csv.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIsADirectoryError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     23\u001b[39m         f.write(response.content)\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m raw_csv_path.exists():\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mgzip\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mraw_gz_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m f_in:\n\u001b[32m     27\u001b[39m         \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(raw_csv_path, \u001b[33m\"\u001b[39m\u001b[33mwb\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f_out:\n\u001b[32m     28\u001b[39m             shutil.copyfileobj(f_in, f_out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/gzip.py:66\u001b[39m, in \u001b[36mopen\u001b[39m\u001b[34m(filename, mode, compresslevel, encoding, errors, newline)\u001b[39m\n\u001b[32m     64\u001b[39m gz_mode = mode.replace(\u001b[33m\"\u001b[39m\u001b[33mt\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filename, (\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mbytes\u001b[39m, os.PathLike)):\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m     binary_file = \u001b[43mGzipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgz_mode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcompresslevel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mread\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(filename, \u001b[33m\"\u001b[39m\u001b[33mwrite\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m     68\u001b[39m     binary_file = GzipFile(\u001b[38;5;28;01mNone\u001b[39;00m, gz_mode, compresslevel, filename)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/gzip.py:203\u001b[39m, in \u001b[36mGzipFile.__init__\u001b[39m\u001b[34m(self, filename, mode, compresslevel, fileobj, mtime)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m fileobj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         fileobj = \u001b[38;5;28mself\u001b[39m.myfileobj = \u001b[43mbuiltins\u001b[49m\u001b[43m.\u001b[49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mrb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m filename \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    205\u001b[39m         filename = \u001b[38;5;28mgetattr\u001b[39m(fileobj, \u001b[33m'\u001b[39m\u001b[33mname\u001b[39m\u001b[33m'\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mIsADirectoryError\u001b[39m: [Errno 21] Is a directory: '/Users/christianmulder/Desktop/CASA/FSDS_Groupwork/01_Data/Raw/20250615-London-listings.csv.gz'"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Part 1: Download, Save, and Unzip Raw Data\n",
    "# ==========================================\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://orca.casa.ucl.ac.uk/~jreades/data/20250615-London-listings.csv.gz\"\n",
    "\n",
    "base_folder = Path.cwd()\n",
    "raw_folder = base_folder / \"01_Data/Raw\"\n",
    "raw_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_gz_path = raw_folder / Path(url).name\n",
    "raw_csv_path = raw_gz_path.with_suffix(\"\")\n",
    "\n",
    "if not raw_gz_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(raw_gz_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "if not raw_csv_path.exists():\n",
    "    with gzip.open(raw_gz_path, \"rb\") as f_in:\n",
    "        with open(raw_csv_path, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "#df = pd.read_csv(raw_csv_path)\n",
    "pd.read_csv(raw_csv_path).to_parquet(raw_csv_path.with_suffix(\".parquet\"))\n",
    "                                     \n",
    "# ========================\n",
    "# Part 2: Reducing Columns\n",
    "# ========================\n",
    "\n",
    "cols = ['id', 'listing_url', 'last_scraped', 'name', \n",
    "    'description', 'host_id', 'host_name', 'host_since', \n",
    "    'host_location', 'host_about', 'host_is_superhost', \n",
    "    'host_listings_count', 'host_total_listings_count', \n",
    "    'host_verifications', 'latitude', 'longitude', \n",
    "    'property_type', 'room_type', 'accommodates', \n",
    "    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', \n",
    "    'amenities', 'price', 'minimum_nights', 'maximum_nights', \n",
    "    'availability_365', 'number_of_reviews', \n",
    "    'first_review', 'last_review', 'review_scores_rating', \n",
    "    'license', 'reviews_per_month', 'estimated_occupancy_l365d', \n",
    "    'estimated_revenue_l365d', 'number_of_reviews_ltm']\n",
    "\n",
    "df = pd.read_parquet(raw_csv_path.with_suffix(\".parquet\"), columns=cols)\n",
    "\n",
    "# Set to show ALL columns without truncation\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)  # Prevent line wrapping\n",
    "\n",
    "# ========================\n",
    "# Part 3: Null Values\n",
    "# ========================\n",
    "\n",
    "# drop the columns which contain too many nans\n",
    "df.drop(columns=['license','host_about'], inplace=True)\n",
    "\n",
    "# Count rows by N/A values\n",
    "probs = df.isnull().sum(axis=1)\n",
    "\n",
    "# Optionally create a histogram but do not display it\n",
    "probs.plot.hist(bins=30).get_figure().clf()  # closes figure to prevent output\n",
    "\n",
    "# drop rows with more than 5 nans\n",
    "cutoff = 5\n",
    "df.drop(probs[probs > cutoff].index, inplace=True)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Part 4: Fixing Data Types\n",
    "# ==============================\n",
    "\n",
    "# Boolean type data\n",
    "bools = ['host_is_superhost']\n",
    "for b in bools:\n",
    "    df[b] = df[b].replace({'f': False, 't': True}).astype('bool')\n",
    "    \n",
    "# Date type data\n",
    "dates = ['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "for d in dates:\n",
    "    df[d] = pd.to_datetime(df[d])\n",
    "    \n",
    "# Categories type\n",
    "cats = ['property_type', 'room_type']\n",
    "for c in cats:\n",
    "    df[c] = df[c].astype('category')\n",
    "    \n",
    "# Strings type (price)\n",
    "money = ['price']\n",
    "for m in money:\n",
    "    try:\n",
    "        df[m] = (\n",
    "            df[m].astype(str)                       # force to string\n",
    "                 .str.replace(\"$\", \"\", regex=False) # remove dollar signs\n",
    "                 .str.replace(\",\", \"\", regex=False) # remove commas\n",
    "                 .astype(float)                     # convert to float\n",
    "        )\n",
    "    except (ValueError, AttributeError):\n",
    "        pass  # silently ignore conversion errors\n",
    "    \n",
    "# Integer type\n",
    "ints = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',\n",
    "        'beds','minimum_nights','maximum_nights','availability_365']\n",
    "for i in ints:\n",
    "    try:\n",
    "        df[i] = df[i].astype('float').astype('int')\n",
    "    except ValueError:\n",
    "        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())\n",
    "        \n",
    "# Reset index after cleaning\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Part 5: Storing Cleaned Data\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define output paths\n",
    "csv_out = Path(\"01_Data/Cleaned/listings.csv\")\n",
    "pq_out  = Path(\"01_Data/Cleaned/listings.parquet\")\n",
    "csv_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save CSV\n",
    "df.to_csv(csv_out, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Save Parquet using fastparquet\n",
    "df.to_parquet(pq_out, engine=\"fastparquet\", index=False)\n",
    "\n",
    "del(df)\n",
    "\n",
    "# Load cleaned data silently\n",
    "df_cleaned = pd.read_parquet(pq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444e74f",
   "metadata": {},
   "source": [
    "If all went well, this should create two folders in your working directory: `raw_data` and `cleaned_data`, containing the respective CSV files. You can load the cleaned data into a pandas DataFrame as follows: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
