---
title: "Spatial Analysis of Multi‑Host Airbnb Activity in London"
author: "Christian Mulder"
affiliation: "University College London (UCL), MSc Urban Spatial Science"
date: "November 2025"
documentclass: article
fontsize: 12pt
linestretch: 1.5
geometry: margin=1in
mainfont: Times New Roman
sansfont: Arial
monofont: Courier New
execute:
  echo: false     # hide code
  warning: false  # hide warnings
  error: false    # hide errors
header-includes:
  - \usepackage{setspace}
  - \usepackage{sectsty}
  - \sectionfont{\normalsize}
---
Loading in the data: 
```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt

# --- Load tabular listings data ---
df_l = pd.read_csv("listings.csv")

# --- Load London boundary shapefiles ---
boroughs = gpd.read_file(
    "statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp"
)
msoas = gpd.read_file(
    "statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp"
)

# --- Convert listings to GeoDataFrame ---
gdf_l = gpd.GeoDataFrame(
    df_l,
    geometry=gpd.points_from_xy(df_l.longitude, df_l.latitude),
    crs="EPSG:4326"  # WGS84
).to_crs(epsg=27700)  # British National Grid

# --- Identify professional hosts (>=2 listings) ---
host_counts = df_l["host_id"].value_counts()
multi_hosts = host_counts[host_counts >= 2].index
df_multi_hosts = df_l[df_l["host_id"].isin(multi_hosts)]

# --- Convert professional hosts to GeoDataFrame ---
gdf_multi_hosts = gpd.GeoDataFrame(
    df_multi_hosts,
    geometry=gpd.points_from_xy(df_multi_hosts.longitude, df_multi_hosts.latitude),
    crs="EPSG:4326"
).to_crs(epsg=27700)

# --- Map 1: All listings over borough + MSOA boundaries ---
fig, ax = plt.subplots(figsize=(12, 12))
boroughs.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=1)
msoas.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=0.5)
gdf_l.plot(ax=ax, markersize=2, color="blue", alpha=0.6)
ax.set_title("All Listings over Borough (red) and MSOA (black) Boundaries", fontsize=14)
ax.axis("off")
plt.show()

# --- Map 2: Professional hosts (>=2 listings) over borough + MSOA boundaries ---
fig, ax = plt.subplots(figsize=(12, 12))
boroughs.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=1)
msoas.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=0.5)
gdf_multi_hosts.plot(ax=ax, markersize=2, color="green", alpha=0.6)
ax.set_title("Professional Hosts (>=2 listings) over Borough (red) and MSOA (black) Boundaries", fontsize=14)
ax.axis("off")
plt.show()
```
```{python}
import numpy as np
import matplotlib.pyplot as plt

# --- Clean and compute incomes (all listings) ---
df_l = df_l.copy()
df_l["price"] = pd.to_numeric(df_l["price"], errors="coerce")
df_l["estimated_occupancy_l365d"] = pd.to_numeric(df_l["estimated_occupancy_l365d"], errors="coerce")

# drop invalid/zero
df_l = df_l.dropna(subset=["price", "estimated_occupancy_l365d"])
df_l = df_l[(df_l["price"] > 0) & (df_l["estimated_occupancy_l365d"] > 0)]

# income and winsorize (clip extremes to 99th percentile)
df_l["income_estimate"] = df_l["price"] * df_l["estimated_occupancy_l365d"]
p99_all = np.percentile(df_l["income_estimate"], 99)
df_l["income_w"] = np.clip(df_l["income_estimate"], None, p99_all)

# --- Clean and compute incomes (multi-hosts) ---
df_multi_hosts = df_multi_hosts.copy()
df_multi_hosts["price"] = pd.to_numeric(df_multi_hosts["price"], errors="coerce")
df_multi_hosts["estimated_occupancy_l365d"] = pd.to_numeric(df_multi_hosts["estimated_occupancy_l365d"], errors="coerce")

df_multi_hosts = df_multi_hosts.dropna(subset=["price", "estimated_occupancy_l365d"])
df_multi_hosts = df_multi_hosts[(df_multi_hosts["price"] > 0) & (df_multi_hosts["estimated_occupancy_l365d"] > 0)]

df_multi_hosts["income_estimate"] = df_multi_hosts["price"] * df_multi_hosts["estimated_occupancy_l365d"]
p99_multi = np.percentile(df_multi_hosts["income_estimate"], 99)
df_multi_hosts["income_w"] = np.clip(df_multi_hosts["income_estimate"], None, p99_multi)

# --- Graph 1: Log-scale histogram (All listings) ---
plt.figure(figsize=(10,6))
# log-spaced bins from min>0 to 99th percentile
min_all = max(df_l["income_w"].min(), 1)
bins_all = np.logspace(np.log10(min_all), np.log10(p99_all), 40)
plt.hist(df_l["income_w"], bins=bins_all, color="skyblue", edgecolor="black")
plt.xscale("log")
plt.title("Distribution of Estimated Income (All Listings, log scale, 99th pct clipped)")
plt.xlabel("Estimated Annual Income (£, log scale)")
plt.ylabel("Number of Listings")
plt.tight_layout()
plt.show()

# --- Graph 2: Log-scale histogram (Multi-host listings) ---
plt.figure(figsize=(10,6))
min_multi = max(df_multi_hosts["income_w"].min(), 1)
bins_multi = np.logspace(np.log10(min_multi), np.log10(p99_multi), 40)
plt.hist(df_multi_hosts["income_w"], bins=bins_multi, color="green", edgecolor="black")
plt.xscale("log")
plt.title("Distribution of Estimated Income (Multi-Host Listings, log scale, 99th pct clipped)")
plt.xlabel("Estimated Annual Income (£, log scale)")
plt.ylabel("Number of Listings")
plt.tight_layout()
plt.show()
```

Mapping the average income now. I chose average (): 

```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
import numpy as np

# --- Load tabular listings data ---
df_l = pd.read_csv("listings.csv")

# --- Load London boundary shapefiles ---
boroughs = gpd.read_file(
    "statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp"
).to_crs(epsg=27700)
msoas = gpd.read_file(
    "statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp"
).to_crs(epsg=27700)

# --- Convert listings to GeoDataFrame ---
gdf_l = gpd.GeoDataFrame(
    df_l,
    geometry=gpd.points_from_xy(df_l.longitude, df_l.latitude),
    crs="EPSG:4326"
).to_crs(epsg=27700)

# --- Compute estimated income (winsorized at 99th percentile) ---
gdf_l["price"] = pd.to_numeric(gdf_l["price"], errors="coerce")
gdf_l["estimated_occupancy_l365d"] = pd.to_numeric(gdf_l["estimated_occupancy_l365d"], errors="coerce")
gdf_l = gdf_l.dropna(subset=["price", "estimated_occupancy_l365d"])
gdf_l = gdf_l[(gdf_l["price"] > 0) & (gdf_l["estimated_occupancy_l365d"] > 0)]

gdf_l["income_estimate"] = gdf_l["price"] * gdf_l["estimated_occupancy_l365d"]
p99 = np.percentile(gdf_l["income_estimate"], 99)
gdf_l["income_w"] = np.clip(gdf_l["income_estimate"], None, p99)

# --- Spatial join: listings to MSOAs ---
gdf_msoa_join = gpd.sjoin(gdf_l, msoas, how="inner", predicate="within")

# Aggregate mean/median income per MSOA
msoa_income = gdf_msoa_join.groupby("MSOA11CD")["income_w"].agg(["mean", "median"]).reset_index()

# Merge back to MSOA polygons
msoas_income = msoas.merge(msoa_income, on="MSOA11CD", how="left")

# --- Spatial join: listings to Boroughs ---
gdf_borough_join = gpd.sjoin(gdf_l, boroughs, how="inner", predicate="within")

borough_income = gdf_borough_join.groupby("GSS_CODE")["income_w"].agg(["mean", "median"]).reset_index()
boroughs_income = boroughs.merge(borough_income, on="GSS_CODE", how="left")

# --- Map 1: Average income per MSOA (median) ---
fig, ax = plt.subplots(figsize=(12, 12))
msoas_income.plot(column="median", cmap="viridis", legend=True, ax=ax,
                  legend_kwds={"label": "Median Estimated Income (£)"})
boroughs.boundary.plot(ax=ax, color="red", linewidth=1)
ax.set_title("Median Estimated Income per MSOA", fontsize=14)
ax.axis("off")
plt.show()

# --- Map 2: Average income per Borough (median) ---
fig, ax = plt.subplots(figsize=(12, 12))
boroughs_income.plot(column="median", cmap="plasma", legend=True, ax=ax,
                     legend_kwds={"label": "Median Estimated Income (£)"})
boroughs.boundary.plot(ax=ax, color="black", linewidth=1)
ax.set_title("Median Estimated Income per Borough", fontsize=14)
ax.axis("off")
plt.show()
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, kurtosis

# --- Load listings data ---
df_l = pd.read_csv("listings.csv")

# --- Clean and compute winsorized income ---
df_l["price"] = pd.to_numeric(df_l["price"], errors="coerce")
df_l["estimated_occupancy_l365d"] = pd.to_numeric(df_l["estimated_occupancy_l365d"], errors="coerce")
df_l = df_l.dropna(subset=["price", "estimated_occupancy_l365d"])
df_l = df_l[(df_l["price"] > 0) & (df_l["estimated_occupancy_l365d"] > 0)]

df_l["income_estimate"] = df_l["price"] * df_l["estimated_occupancy_l365d"]
p99 = np.percentile(df_l["income_estimate"], 99)
df_l["income_w"] = np.clip(df_l["income_estimate"], None, p99)

# --- Log-transform ---
df_l["log_income"] = np.log(df_l["income_w"] + 1)

# --- Compute stats ---
skew_raw = skew(df_l["income_w"])
kurt_raw = kurtosis(df_l["income_w"])
skew_log = skew(df_l["log_income"])
kurt_log = kurtosis(df_l["log_income"])

# --- Plot raw income histogram ---
plt.figure(figsize=(10,6))
sns.histplot(df_l["income_w"], bins=50, kde=True, color="skyblue")
plt.title("Income Distribution (Raw Winsorized)")
plt.xlabel("Estimated Annual Income (£)")
plt.ylabel("Number of Listings")
plt.text(0.95, 0.95,
         f"Skewness: {skew_raw:.2f}\nKurtosis: {kurt_raw:.2f}",
         transform=plt.gca().transAxes,
         ha="right", va="top",
         bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))
plt.show()

# --- Plot log-transformed income histogram ---
plt.figure(figsize=(10,6))
sns.histplot(df_l["log_income"], bins=50, kde=True, color="green")
plt.title("Income Distribution (Log-Transformed)")
plt.xlabel("Log(Estimated Annual Income + 1)")
plt.ylabel("Number of Listings")
plt.text(0.95, 0.95,
         f"Skewness: {skew_log:.2f}\nKurtosis: {kurt_log:.2f}",
         transform=plt.gca().transAxes,
         ha="right", va="top",
         bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))
plt.show()
```

This code cleans the listings data, computes winsorized annual income, and then plots its distribution on both raw and log scales. The log‑scaled view makes the skew much clearer, showing that—similar to the distribution of counts—there are a few areas of London with significantly higher estimated annual incomes, which also aligns with the spatial patterns seen in the maps.

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, kurtosis

# --- Load listings data ---
df_l = pd.read_csv("listings.csv")

# --- Clean and compute winsorized income per listing ---
df_l["price"] = pd.to_numeric(df_l["price"], errors="coerce")
df_l["estimated_occupancy_l365d"] = pd.to_numeric(df_l["estimated_occupancy_l365d"], errors="coerce")
df_l = df_l.dropna(subset=["price", "estimated_occupancy_l365d"])
df_l = df_l[(df_l["price"] > 0) & (df_l["estimated_occupancy_l365d"] > 0)]

df_l["income_estimate"] = df_l["price"] * df_l["estimated_occupancy_l365d"]
p99 = np.percentile(df_l["income_estimate"], 99)
df_l["income_w"] = np.clip(df_l["income_estimate"], None, p99)

# --- Aggregate to host level (gross annual income per user ID) ---
host_income = df_l.groupby("host_id")["income_w"].sum().reset_index()
host_income.rename(columns={"income_w": "gross_income"}, inplace=True)

# --- Compute stats ---
skew_host = skew(host_income["gross_income"])
kurt_host = kurtosis(host_income["gross_income"])

print("Host-level gross income distribution")
print("Skewness:", skew_host)
print("Kurtosis:", kurt_host)

# --- Histogram (linear scale) ---
plt.figure(figsize=(10,6))
sns.histplot(host_income["gross_income"], bins=50, kde=True, color="skyblue")
plt.title("Gross Annual Income per Host (linear scale)")
plt.xlabel("Gross Annual Income (£)")
plt.ylabel("Number of Hosts")
plt.text(0.95, 0.95,
         f"Skewness: {skew_host:.2f}\nKurtosis: {kurt_host:.2f}",
         transform=plt.gca().transAxes,
         ha="right", va="top",
         bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))
plt.show()

# --- Histogram (log scale) ---
plt.figure(figsize=(10,6))
sns.histplot(host_income["gross_income"], bins=50, kde=True, color="green", log_scale=True)
plt.title("Gross Annual Income per Host (log scale)")
plt.xlabel("Gross Annual Income (£, log scale)")
plt.ylabel("Number of Hosts")
plt.text(0.95, 0.95,
         f"Skewness: {skew_host:.2f}\nKurtosis: {kurt_host:.2f}",
         transform=plt.gca().transAxes,
         ha="right", va="top",
         bbox=dict(boxstyle="round", facecolor="white", alpha=0.8))
plt.show()
```

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from scipy.stats import skew, kurtosis
from matplotlib.ticker import FuncFormatter

# --- Load ---
df = pd.read_csv("listings.csv")

# --- Robust cleaning ---
# Clean price strings like "£120", "1,200", " 85 " -> float
df["price"] = (
    df["price"]
    .astype(str)
    .str.replace(r"[^\d\.]", "", regex=True)
    .replace("", np.nan)
    .astype(float)
)

# Ensure occupancy numeric
df["estimated_occupancy_l365d"] = pd.to_numeric(df["estimated_occupancy_l365d"], errors="coerce")

# Drop missing
df = df.dropna(subset=["price", "estimated_occupancy_l365d"])

# --- Filter to active listings (>=90 days) and positive price ---
active = df[(df["estimated_occupancy_l365d"] >= 90) & (df["price"] > 0)].copy()

# --- Compute per-listing income and winsorize ---
active["income_estimate"] = active["price"] * active["estimated_occupancy_l365d"]
# Remove any accidental non-positive incomes
active = active[active["income_estimate"] > 0]

p99 = np.percentile(active["income_estimate"], 99)
active["income_w"] = np.clip(active["income_estimate"], None, p99)

# --- Aggregate to host-level gross income ---
host_income = (
    active.groupby("host_id", as_index=False)["income_w"]
    .sum()
    .rename(columns={"income_w": "gross_income"})
)

# Drop any hosts with non-positive gross income (should be none after cleaning)
host_income = host_income[host_income["gross_income"] > 0].copy()

# --- Diagnostics (optional) ---
print(f"Active listings: {len(active):,}")
print(f"Active hosts: {len(host_income):,}")
zeros = (host_income["gross_income"] <= 0).sum()
print(f"Hosts with zero/non-positive income (should be 0): {zeros}")

# --- Stats ---
skew_val = skew(host_income["gross_income"])
kurt_val = kurtosis(host_income["gross_income"])

# --- Formatters for currency ticks ---
def pounds(x, pos):
    return f"£{int(x):,}"

def pounds_abbrev(x, pos):
    if x >= 1_000_000:
        return f"£{x/1_000_000:.0f}M"
    elif x >= 1_000:
        return f"£{x/1_000:.0f}K"
    else:
        return f"£{int(x)}"

# --- Linear histogram with readable currency axis (cap at 99th percentile) ---
plt.figure(figsize=(10,6))
x_min = host_income["gross_income"].min()
x_max = host_income["gross_income"].quantile(0.99)
bins_linear = np.linspace(x_min, x_max, 50)
sns.histplot(host_income["gross_income"], bins=bins_linear, kde=True, color="skyblue")
ax = plt.gca()
ax.xaxis.set_major_formatter(FuncFormatter(pounds))
plt.title("Gross Annual Income per Active Host (linear scale)")
plt.xlabel("Gross Annual Income (£)")
plt.ylabel("Number of Hosts")
plt.text(0.98, 0.98,
         f"Skewness: {skew_val:.2f}\nKurtosis: {kurt_val:.2f}",
         transform=ax.transAxes, ha="right", va="top",
         bbox=dict(boxstyle="round", facecolor="white", alpha=0.85))
plt.tight_layout()
plt.show()

# --- Log histogram with log-spaced bins and abbreviated currency ticks ---
plt.figure(figsize=(10,6))
min_val = max(1, host_income["gross_income"].min())
x_max_log = max(min_val, host_income["gross_income"].quantile(0.99))
bins_log = np.logspace(np.log10(min_val), np.log10(x_max_log), 50)
sns.histplot(host_income["gross_income"], bins=bins_log, kde=True, color="green")
ax = plt.gca()
ax.set_xscale("log")
ax.xaxis.set_major_formatter(FuncFormatter(pounds_abbrev))
plt.title("Gross Annual Income per Active Host (log scale)")
plt.xlabel("Gross Annual Income (£, log scale)")
plt.ylabel("Number of Hosts")
plt.text(0.98, 0.98,
         f"Skewness: {skew_val:.2f}\nKurtosis: {kurt_val:.2f}",
         transform=ax.transAxes, ha="right", va="top",
         bbox=dict(boxstyle="round", facecolor="white", alpha=0.85))
plt.tight_layout()
plt.show()
```

```{python}
# --- Group by borough and compute mean income ---
borough_income = gdf_borough_join.groupby("GSS_CODE")["income_w"].mean().reset_index()
borough_income = borough_income.merge(boroughs[["GSS_CODE", "NAME"]], on="GSS_CODE", how="left")

# --- Find highest average income ---
top_borough = borough_income.sort_values("income_w", ascending=False).iloc[0]
print(f"Highest average income: {top_borough['NAME']} (£{top_borough['income_w']:.2f})")
```

```{python}
median_income = gdf_borough_join.groupby("GSS_CODE")["income_w"].median().reset_index()
median_income = median_income.merge(boroughs[["GSS_CODE", "NAME"]], on="GSS_CODE", how="left")
top_median = median_income.sort_values("income_w", ascending=False).iloc[0]
print(f"Highest median income: {top_median['NAME']} (£{top_median['income_w']:.2f})")
```

```{python}
listing_counts = gdf_borough_join["GSS_CODE"].value_counts().reset_index()
listing_counts.columns = ["GSS_CODE", "count"]
listing_counts = listing_counts.merge(boroughs[["GSS_CODE", "NAME"]], on="GSS_CODE", how="left")
top_count = listing_counts.sort_values("count", ascending=False).iloc[0]
print(f"Borough with most listings: {top_count['NAME']} ({top_count['count']} listings)")
```

```{python}
total_income = gdf_borough_join.groupby("GSS_CODE")["income_w"].sum().reset_index()
total_income = total_income.merge(boroughs[["GSS_CODE", "NAME"]], on="GSS_CODE", how="left")
top_total = total_income.sort_values("income_w", ascending=False).iloc[0]
print(f"Highest total income: {top_total['NAME']} (£{top_total['income_w']:.2f})")
```

```{python}
import pandas as pd
import numpy as np

# --- Load listings ---
df = pd.read_csv("listings.csv")

# --- Clean numeric fields ---
df["price"] = pd.to_numeric(df["price"], errors="coerce")
df["estimated_occupancy_l365d"] = pd.to_numeric(df["estimated_occupancy_l365d"], errors="coerce")
df["bedrooms"] = pd.to_numeric(df["bedrooms"], errors="coerce")

# --- Drop invalids ---
df = df.dropna(subset=["price", "estimated_occupancy_l365d", "bedrooms"])
df = df[(df["price"] > 0) & (df["estimated_occupancy_l365d"] > 0) & (df["bedrooms"] > 0)]

# --- Compute income per listing ---
df["income_estimate"] = df["price"] * df["estimated_occupancy_l365d"]

# Winsorize at 99th percentile
p99 = np.percentile(df["income_estimate"], 99)
df["income_w"] = np.clip(df["income_estimate"], None, p99)

# --- Normalize by bedrooms ---
df["income_per_bedroom"] = df["income_w"] / df["bedrooms"]

# --- Spatial join to boroughs (assuming you already have gdf listings + boroughs) ---
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.longitude, df.latitude),
    crs="EPSG:4326"
).to_crs(epsg=27700)

gdf_borough = gpd.sjoin(gdf, boroughs, how="inner", predicate="within")

# --- Aggregate per borough ---
borough_income_per_bedroom = (
    gdf_borough.groupby("GSS_CODE")["income_per_bedroom"]
    .mean()
    .reset_index()
    .merge(boroughs[["GSS_CODE", "NAME"]], on="GSS_CODE", how="left")
)

# --- Find top borough ---
top = borough_income_per_bedroom.sort_values("income_per_bedroom", ascending=False).iloc[0]
print(f"Highest average income per bedroom: {top['NAME']} (£{top['income_per_bedroom']:.2f})")
```


```{python}
import pandas as pd
import geopandas as gpd

# --- Load listings ---
df = pd.read_csv("listings.csv")

# --- Convert to GeoDataFrame ---
gdf = gpd.GeoDataFrame(
    df,
    geometry=gpd.points_from_xy(df.longitude, df.latitude),
    crs="EPSG:4326"
).to_crs(epsg=27700)  # project to meters for distance checks

# --- Round coordinates to ~10m grid ---
# (10m tolerance: divide by 10, round, then multiply back)
gdf["x_round"] = (gdf.geometry.x / 10).round() * 10
gdf["y_round"] = (gdf.geometry.y / 10).round() * 10

# --- Define "address cluster" as host_id + rounded coords ---
gdf["address_cluster"] = (
    gdf["host_id"].astype(str) + "_" +
    gdf["x_round"].astype(str) + "_" +
    gdf["y_round"].astype(str)
)

# --- Count listings per cluster ---
cluster_counts = gdf.groupby("address_cluster").size().reset_index(name="count")

# --- Summarize how many clusters have 2+, 3+, 5+ listings ---
n2 = (cluster_counts["count"] >= 2).sum()
n3 = (cluster_counts["count"] >= 3).sum()
n5 = (cluster_counts["count"] >= 5).sum()

print(f"Clusters with ≥2 listings: {n2}")
print(f"Clusters with ≥3 listings: {n3}")
print(f"Clusters with ≥5 listings: {n5}")
```

These results show that duplication at the same address is widespread: 1,806 clusters have at least two listings by the same host, 705 clusters have three or more, and 248 clusters have five or more. This concentration of multiple listings at identical locations underscores how Airbnb is dominated by professional operators rather than casual hosts, reinforcing the argument that the platform is out of control.