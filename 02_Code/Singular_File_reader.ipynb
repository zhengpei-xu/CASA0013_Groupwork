{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f33984ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ==========================================\n",
    "# Part 1: Download, Save, and Unzip Raw Data\n",
    "# ==========================================\n",
    "import requests\n",
    "from pathlib import Path\n",
    "import gzip\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "url = \"https://orca.casa.ucl.ac.uk/~jreades/data/20250615-London-listings.csv.gz\"\n",
    "\n",
    "base_folder = Path.cwd()\n",
    "raw_folder = base_folder / \"01_Data/Raw\"\n",
    "raw_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "raw_gz_path = raw_folder / Path(url).name\n",
    "raw_csv_path = raw_gz_path.with_suffix(\"\")\n",
    "\n",
    "if not raw_gz_path.exists():\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    with open(raw_gz_path, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "if not raw_csv_path.exists():\n",
    "    with gzip.open(raw_gz_path, \"rb\") as f_in:\n",
    "        with open(raw_csv_path, \"wb\") as f_out:\n",
    "            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "#df = pd.read_csv(raw_csv_path)\n",
    "pd.read_csv(raw_csv_path).to_parquet(raw_csv_path.with_suffix(\".parquet\"))\n",
    "                                     \n",
    "# ========================\n",
    "# Part 2: Reducing Columns\n",
    "# ========================\n",
    "\n",
    "cols = ['id', 'listing_url', 'last_scraped', 'name', \n",
    "    'description', 'host_id', 'host_name', 'host_since', \n",
    "    'host_location', 'host_about', 'host_is_superhost', \n",
    "    'host_listings_count', 'host_total_listings_count', \n",
    "    'host_verifications', 'latitude', 'longitude', \n",
    "    'property_type', 'room_type', 'accommodates', \n",
    "    'bathrooms', 'bathrooms_text', 'bedrooms', 'beds', \n",
    "    'amenities', 'price', 'minimum_nights', 'maximum_nights', \n",
    "    'availability_365', 'number_of_reviews', \n",
    "    'first_review', 'last_review', 'review_scores_rating', \n",
    "    'license', 'reviews_per_month', 'estimated_occupancy_l365d', \n",
    "    'estimated_revenue_l365d', 'number_of_reviews_ltm']\n",
    "\n",
    "df = pd.read_parquet(raw_csv_path.with_suffix(\".parquet\"), columns=cols)\n",
    "\n",
    "# Set to show ALL columns without truncation\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)  # Prevent line wrapping\n",
    "\n",
    "# ========================\n",
    "# Part 3: Null Values\n",
    "# ========================\n",
    "\n",
    "# drop the columns which contain too many nans\n",
    "df.drop(columns=['license','host_about'], inplace=True)\n",
    "\n",
    "# Count rows by N/A values\n",
    "probs = df.isnull().sum(axis=1)\n",
    "\n",
    "# Optionally create a histogram but do not display it\n",
    "probs.plot.hist(bins=30).get_figure().clf()  # closes figure to prevent output\n",
    "\n",
    "# drop rows with more than 5 nans\n",
    "cutoff = 5\n",
    "df.drop(probs[probs > cutoff].index, inplace=True)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Part 4: Fixing Data Types\n",
    "# ==============================\n",
    "\n",
    "# Boolean type data\n",
    "bools = ['host_is_superhost']\n",
    "for b in bools:\n",
    "    df[b] = df[b].replace({'f': False, 't': True}).astype('bool')\n",
    "    \n",
    "# Date type data\n",
    "dates = ['last_scraped', 'host_since', 'first_review', 'last_review']\n",
    "for d in dates:\n",
    "    df[d] = pd.to_datetime(df[d])\n",
    "    \n",
    "# Categories type\n",
    "cats = ['property_type', 'room_type']\n",
    "for c in cats:\n",
    "    df[c] = df[c].astype('category')\n",
    "    \n",
    "# Strings type (price)\n",
    "money = ['price']\n",
    "for m in money:\n",
    "    try:\n",
    "        df[m] = (\n",
    "            df[m].astype(str)                       # force to string\n",
    "                 .str.replace(\"$\", \"\", regex=False) # remove dollar signs\n",
    "                 .str.replace(\",\", \"\", regex=False) # remove commas\n",
    "                 .astype(float)                     # convert to float\n",
    "        )\n",
    "    except (ValueError, AttributeError):\n",
    "        pass  # silently ignore conversion errors\n",
    "    \n",
    "# Integer type\n",
    "ints = ['id','host_id','host_listings_count','host_total_listings_count','accommodates',\n",
    "        'beds','minimum_nights','maximum_nights','availability_365']\n",
    "for i in ints:\n",
    "    try:\n",
    "        df[i] = df[i].astype('float').astype('int')\n",
    "    except ValueError:\n",
    "        df[i] = df[i].astype('float').astype(pd.UInt16Dtype())\n",
    "        \n",
    "# Reset index after cleaning\n",
    "df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "# ==============================\n",
    "# Part 5: Storing Cleaned Data\n",
    "# ==============================\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Define output paths\n",
    "csv_out = Path(\"01_Data/Cleaned/listings.csv\")\n",
    "pq_out  = Path(\"01_Data/Cleaned/listings.parquet\")\n",
    "csv_out.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save CSV\n",
    "df.to_csv(csv_out, index=False, encoding=\"utf-8\")\n",
    "\n",
    "# Save Parquet using fastparquet\n",
    "df.to_parquet(pq_out, engine=\"fastparquet\", index=False)\n",
    "\n",
    "del(df)\n",
    "\n",
    "# Load cleaned data silently\n",
    "df_cleaned = pd.read_parquet(pq_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8444e74f",
   "metadata": {},
   "source": [
    "If all went well, this should create two folders in your working directory: `raw_data` and `cleaned_data`, containing the respective CSV files. You can load the cleaned data into a pandas DataFrame as follows: "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
