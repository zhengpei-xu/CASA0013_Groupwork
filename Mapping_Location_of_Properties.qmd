---
title: "Spatial Analysis of Multi‑Host Airbnb Activity in London"
jupyter: python3
author: "Christian Mulder"
affiliation: "University College London (UCL), MSc Urban Spatial Science"
date: "November 2025"
documentclass: article
fontsize: 12pt
linestretch: 1.5
geometry: margin=1in
mainfont: Times New Roman
sansfont: Arial
monofont: Courier New
execute:
  echo: false     # hide code
  warning: false  # hide warnings
  error: false    # hide errors
header-includes:
  - \usepackage{setspace}
  - \usepackage{sectsty}
  - \sectionfont{\normalsize}
format:
  pdf:
    fig-width: 6.5
    fig-height: 4.5
    fig-pos: "H"
    fig-align: center
    keep-figures: true
---

To begin with, I will start by reading in all the necessary files, which are the listings CSV, the borough boundary data, and then the MSOA data.
```{python}
import pandas as pd
import geopandas as gpd
import matplotlib.pyplot as plt
from pathlib import Path

# --- Define new file paths ---
listings_path = Path("01_Data/Cleaned/listings.csv")
boroughs_path = Path("statistical-gis-boundaries-london/ESRI/London_Borough_Excluding_MHW.shp")
msoas_path    = Path("statistical-gis-boundaries-london/ESRI/MSOA_2011_London_gen_MHW.shp")

# --- Load tabular listings data ---
df_l = pd.read_csv(listings_path)

# --- Load London boundary shapefiles ---
boroughs = gpd.read_file(boroughs_path)
msoas = gpd.read_file(msoas_path)

# --- Convert listings to GeoDataFrame ---
gdf_l = gpd.GeoDataFrame(
    df_l,
    geometry=gpd.points_from_xy(df_l.longitude, df_l.latitude),
    crs="EPSG:4326"  # WGS84
).to_crs(epsg=27700)  # British National Grid

# --- Identify professional hosts ---
prof_hosts = df_l[
    (df_l["host_listings_count"] >= 2) & 
    (df_l["availability_365"] >= 300)
]["host_id"].unique()

df_prof_hosts = df_l[df_l["host_id"].isin(prof_hosts)]

# --- Convert professional hosts to GeoDataFrame ---
gdf_prof_hosts = gpd.GeoDataFrame(
    df_prof_hosts,
    geometry=gpd.points_from_xy(df_prof_hosts.longitude, df_prof_hosts.latitude),
    crs="EPSG:4326"
).to_crs(epsg=27700)

# --- Map 1: All listings over borough + MSOA boundaries ---
fig, ax = plt.subplots(figsize=(12, 12))
boroughs.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=1)
msoas.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=0.5)
gdf_l.plot(ax=ax, markersize=2, color="blue", alpha=0.6)
ax.set_title("All Listings over Borough (red) and MSOA (black) Boundaries", fontsize=14)
ax.axis("off")
plt.show()

# --- Map 2: Professional hosts (>=2 listings AND availability >=300 days) ---
fig, ax = plt.subplots(figsize=(12, 12))
boroughs.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=1)
msoas.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=0.5)
gdf_prof_hosts.plot(ax=ax, markersize=2, color="green", alpha=0.6)
ax.set_title(
    "Professional Hosts (>=2 listings AND availability >=300 days) over Borough (red) and MSOA (black) Boundaries",
    fontsize=14
)
ax.axis("off")
plt.show()

print(f"The reduced dataset for professional landlords is {len(gdf_prof_hosts)}")
```

In order to further analyse, I will take these following steps: 
- Counts and distributions (MSOA/borough, normal/log).
- Normality checks (Shapiro–Wilk).
- Gini coefficient (MSOA and borough) → here it acts as a descriptive inequality measure alongside distributions.
- Density visualization (KDE attempt, maps).
- Global spatial autocorrelation (Moran’s I).
- Neighbor definition comparison (KNN vs Distance Band).
- Spatial Error Model.
- *Local clustering (Getis–Ord Gi)**.
- Return to inequality in discussion → you can revisit the Gini in the conclusion, tying it to spatial clustering.

\----------

```{python}
print(df_l.columns)
```

## Counts and Distributions
First, the following two maps will be the distribution of counts (tallied per area) for the Borough and MSOA data.

#####MSOA: 
```{python}
# --- Imports ---
import numpy as np
import matplotlib.pyplot as plt
import geopandas as gpd

# --- MSOA analysis for professional hosts ---
# Spatial join: assign professional listings to MSOAs
gdf_msoa_prof = gpd.sjoin(gdf_prof_hosts, msoas, how="left", predicate="within")

# Aggregate counts per MSOA
prof_counts_msoa = gdf_msoa_prof.groupby("MSOA11NM").size().reset_index(name="prof_count")

# Merge counts back into MSOA polygons
msoas_prof = msoas.merge(prof_counts_msoa, on="MSOA11NM", how="left")
msoas_prof["prof_count"] = msoas_prof["prof_count"].fillna(0)

# Plot MSOA choropleth (raw counts)
fig, ax = plt.subplots(figsize=(12, 12))
msoas_prof.plot(
    column="prof_count",
    cmap="Reds",
    linewidth=0.2,
    edgecolor="black",
    legend=True,
    ax=ax
)
boroughs.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=1)  # borough outlines
ax.set_title("Professional Host Listings per MSOA (boroughs in red)", fontsize=14)
ax.axis("off")
plt.show()
```

##### Borough: 
```{python}
# --- Borough analysis for professional hosts ---
# Spatial join: assign professional listings to boroughs
gdf_borough_prof = gpd.sjoin(gdf_prof_hosts, boroughs, how="left", predicate="within")

# Aggregate counts per borough
prof_counts_borough = gdf_borough_prof.groupby("NAME").size().reset_index(name="prof_count")

# Merge counts back into borough polygons
boroughs_prof = boroughs.merge(prof_counts_borough, on="NAME", how="left")
boroughs_prof["prof_count"] = boroughs_prof["prof_count"].fillna(0)

# Plot borough choropleth (raw counts)
fig, ax = plt.subplots(figsize=(12, 12), constrained_layout=True)
boroughs_prof.plot(
    column="prof_count",
    cmap="Blues",
    linewidth=0.8,
    edgecolor="black",
    legend=True,
    ax=ax
)
ax.set_title("Professional Host Listings per Borough", fontsize=14)
ax.axis("off")
plt.show()
```



#### Distribution table:
```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Extract counts of professional hosts per MSOA
counts = msoas_prof["prof_count"].values

# Compute mean and standard deviation
mean = np.mean(counts)
std = np.std(counts)

# Plot histogram of observed counts
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(counts, bins=100, density=True, alpha=0.6, color="skyblue", edgecolor="black")

# Overlay normal distribution curve
x = np.linspace(counts.min(), counts.max(), 200)
pdf = norm.pdf(x, loc=mean, scale=std)
ax.plot(x, pdf, "r-", lw=2, label="Normal distribution fit")

# Labels and title
ax.set_title("Distribution of Professional Host Listings per MSOA", fontsize=14)
ax.set_xlabel("Professional host count per MSOA")
ax.set_ylabel("Density")
ax.legend()

plt.show()
```

Just to test whether this is a bad skew (although I would say the diagram is pretty obvious...), I will do a Shapiro-Wilk p-value test: 

```{python}
from scipy.stats import shapiro

# Perform Shapiro-Wilk test on professional host counts per MSOA
stat, p = shapiro(msoas_prof["prof_count"])
print(f"Shapiro-Wilk p-value: {p}")
```

Because from this bar chart of the normal distribution, which is very extremely positively skewed, I want to try creating a log-scaled histogram to see if that changes anything. 

```{python}
import numpy as np
import matplotlib.pyplot as plt
from scipy.stats import norm

# Extract counts of professional hosts per MSOA
counts = msoas_prof["prof_count"].values

# Apply log transformation (log(1+x) to avoid log(0))
log_counts = np.log1p(counts)

# Compute mean and standard deviation of log-transformed counts
mean = np.mean(log_counts)
std = np.std(log_counts)

# Plot histogram of log-transformed counts
fig, ax = plt.subplots(figsize=(10, 6))
ax.hist(log_counts, bins=40, density=True, alpha=0.6, color="skyblue", edgecolor="black")

# Overlay normal distribution curve
x = np.linspace(log_counts.min(), log_counts.max(), 200)
pdf = norm.pdf(x, loc=mean, scale=std)
ax.plot(x, pdf, "r-", lw=2, label="Normal distribution fit")

# Labels and title
ax.set_title("Log-Scaled Distribution of Professional Host Listings per MSOA", fontsize=14)
ax.set_xlabel("log(1 + professional host count)")
ax.set_ylabel("Density")
ax.legend()

plt.show()
```

And again let's test the Shapiro-Wilk p-value:
```{python}
from scipy.stats import shapiro

log_counts = np.log1p(msoas_prof["prof_count"])
stat_log, p_log = shapiro(log_counts)
print(f"Shapiro-Wilk p-value (log counts): {p_log}")
```

The Shapiro–Wilk test on the raw MSOA counts returned a p‑value of 2.3\times 10^{-46}, providing overwhelming evidence against normality. This confirms what the histogram suggested: the distribution of multi‑host listings is extremely skewed. After applying a log transformation, the p‑value increased to 0.0300. Although this is still below the conventional 0.05 threshold (meaning the log‑transformed data cannot be considered perfectly normal), the improvement is substantial. The log scale reduces skewness and brings the distribution closer to normality, making it more suitable for visualization and interpretation, especially when comparing areas with smaller counts.

To better visualise the distribution, the following map applies a log scale transformation. This adjustment reduces the dominance of areas with exceptionally high counts (such as Westminster), allowing spatial patterns in other boroughs to remain visible and interpretable.
```{python}
import numpy as np
import matplotlib.pyplot as plt

# ====== MSOA map with log scale ======
# Create a log-transformed column (add 1 to avoid log(0))
msoas_prof["log_prof_count"] = np.log1p(msoas_prof["prof_count"])

fig, ax = plt.subplots(figsize=(12, 12))
msoas_prof.plot(
    column="log_prof_count",
    cmap="Blues",
    linewidth=0.2,
    edgecolor="black",
    legend=True,
    ax=ax
)
boroughs.plot(ax=ax, facecolor="none", edgecolor="red", linewidth=1)  # borough outlines
ax.set_title("Professional Host Listings per MSOA (log scale)", fontsize=14)
ax.axis("off")
plt.show()


# ====== Borough map with log scale ======
boroughs_prof["log_prof_count"] = np.log1p(boroughs_prof["prof_count"])

fig, ax = plt.subplots(figsize=(12, 12))
boroughs_prof.plot(
    column="log_prof_count",
    cmap="Blues",
    linewidth=0.8,
    edgecolor="black",
    legend=True,
    ax=ax
)
ax.set_title("Professional Host Listings per Borough (log scale)", fontsize=14)
ax.axis("off")
plt.show()
```

#### Gini
I will now do a quick Gini test, which measures how unevenly multi‑host listings are distributed across areas. A higher Gini value indicates that activity is concentrated in fewer places, while a lower value suggests a more even spread.

The Gini coefficient ranges from 0 (perfect equality) to 1 (maximum inequality), with values above about 0.5 generally considered high.

```{python}
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Extract borough-level counts ---
counts_borough = boroughs_prof["prof_count"].values  # professional host counts

# --- Step 2: Gini coefficient function ---
def gini(array):
    array = np.sort(array)
    n = len(array)
    cumvals = np.cumsum(array)
    return (n + 1 - 2 * np.sum(cumvals) / cumvals[-1]) / n

# --- Step 3: Calculate Gini ---
gini_borough = gini(counts_borough)

# --- Step 4: Plot sorted distribution ---
sorted_counts = np.sort(counts_borough)
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(range(len(sorted_counts)), sorted_counts, color="darkorange")
ax.set_title("Distribution of Professional Host Listings Across Boroughs", fontsize=14)
ax.set_xlabel("Boroughs (sorted by count)")
ax.set_ylabel("Number of Listings")

# Annotate with Gini value
ax.text(
    0.98, 0.96, f"Gini coefficient:\n{gini_borough:.3f}",
    transform=ax.transAxes,
    fontsize=12,
    verticalalignment='top',
    horizontalalignment='left',
    bbox=dict(facecolor='white', edgecolor='black')
)

plt.tight_layout()
plt.show()
```

The Gini coefficient of 0.503 at the borough scale indicates a moderate degree of inequality in the distribution of multi‑host listings across London. Some boroughs host substantially more multi‑host activity than others, but the overall pattern is less extreme than at finer spatial resolutions.

```{python}
import numpy as np
import matplotlib.pyplot as plt

# --- Step 1: Extract MSOA-level counts ---
counts_msoa = msoas_prof["prof_count"].values  # professional host counts

# --- Step 2: Gini coefficient function ---
def gini(array):
    array = np.sort(array)
    n = len(array)
    cumvals = np.cumsum(array)
    return (n + 1 - 2 * np.sum(cumvals) / cumvals[-1]) / n

# --- Step 3: Calculate Gini ---
gini_msoa = gini(counts_msoa)

# --- Step 4: Plot sorted distribution ---
sorted_counts = np.sort(counts_msoa)
fig, ax = plt.subplots(figsize=(10, 6))
ax.bar(range(len(sorted_counts)), sorted_counts, color="darkorange")
ax.set_title("Distribution of Professional Host Listings Across MSOAs", fontsize=14)
ax.set_xlabel("MSOAs (sorted by count)")
ax.set_ylabel("Number of Listings")

# Annotate with Gini value
ax.text(
    0.98, 0.96, f"Gini coefficient:\n{gini_msoa:.3f}",
    transform=ax.transAxes,
    fontsize=12,
    verticalalignment='top',
    horizontalalignment='left',
    bbox=dict(facecolor='white', edgecolor='black')
)

plt.tight_layout()
plt.show()
```

At the MSOA scale, the Gini coefficient rises to 0.627, revealing a much higher degree of spatial inequality. This shows that inequality is not only present between boroughs, but also concentrated within them: a relatively small number of MSOAs account for a disproportionately large share of multi‑host listings, while the majority contain very few or none. In other words, hotspots of multi‑host activity emerge at the neighbourhood level, highlighting that regulation or policy responses cannot rely solely on borough‑wide averages but must account for highly localized concentrations.

\----------
## Autocorrelation & Point Pattern Analysis 

#### KDE
Kernel Density Estimation (KDE) allows you to move beyond raw counts by creating a smooth surface that highlights where multi‑host listings are most concentrated. This helps reveal localized hotspots and spatial patterns that might be obscured when only looking at aggregated counts. 

```{python} 
import matplotlib.pyplot as plt
import seaborn as sns

# --- Extract projected coordinates for Seaborn ---
x = gdf_prof_hosts.geometry.x
y = gdf_prof_hosts.geometry.y

# --- Plot KDE with borough boundaries ---
fig, ax = plt.subplots(figsize=(12, 12))

# Plot borough outlines
boroughs.plot(ax=ax, facecolor="none", edgecolor="black", linewidth=1.2)

# Seaborn KDE (shaded density only, no points)
sns.kdeplot(
    x=x, y=y,
    fill=True,          # shaded density
    cmap="viridis",
    alpha=0.6,
    bw_adjust=0.5,      # adjust smoothing (lower = more detail, higher = smoother)
    ax=ax
)

ax.set_title("Seaborn KDE of Professional Host Listings", fontsize=14)
ax.set_aspect("equal")
ax.axis("off")
plt.tight_layout()
plt.show()
```

Now the following map has decided on fixd boundaries: 

So I think this simpler map might be better for displaying a simplified version of the KDE considering who we are presenting it to. Howwever, I will need to continue working on this, as the heat map does not seem to correspond somehow with the actual results, so maybe I have made it too smooth and diluted in my attempt to accurately match it to the boundary lines. So it will need some more work. 

Now because I find that both these maps look like they're a little bit off when I'm comparing them to the count (although that is possible in KDE because it is different) I will definitely continue from here. For the remaining autocorrelation I will first do Moran's I:

#### Moran's I Test (Local and Global Outcomes)

```{python}
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from libpysal.weights import KNN
from esda.moran import Moran

# ====== CONFIG ======
MSOA_ID_COL = "MSOA11CD"  # Column for MSOA ID
K_NEIGHBORS = 8            # k for KNN weights (urban grids ~8 is typical)

# 1) Project both datasets to a metric CRS (British National Grid)
msoas = msoas.to_crs(epsg=27700)
gdf_prof_hosts = gdf_prof_hosts.to_crs(msoas.crs)

# 2) Clean MSOA geometries (repair invalid polygons)
msoas = msoas.copy()
msoas["geometry"] = msoas["geometry"].buffer(0)  # repair invalid geometries
msoas = msoas.explode(index_parts=False).reset_index(drop=True)  # ensure single Polygon parts
msoas = msoas[~msoas.is_empty & msoas.geometry.notna()].copy()    # drop empty geometries

# 3) Assign each professional listing to an MSOA and count per MSOA
joined = gpd.sjoin(
    gdf_prof_hosts,
    msoas[[MSOA_ID_COL, "geometry"]],
    predicate="within",
    how="inner"
)
msoa_counts = joined.groupby(MSOA_ID_COL).size().reset_index(name="count")

# Merge counts back to MSOA polygons; fill missing counts with 0
msoas_counted = msoas.merge(msoa_counts, on=MSOA_ID_COL, how="left")
msoas_counted["count"] = msoas_counted["count"].fillna(0).astype(int)
msoas_counted = msoas_counted.reset_index(drop=True)

# 4) Build spatial weights (KNN avoids islands / topology issues)
w = KNN.from_dataframe(msoas_counted, k=K_NEIGHBORS, use_index=True)
w.transform = "r"  # row-standardize for Moran's I

# 5) Compute Moran's I for professional host counts
y = msoas_counted["count"].to_numpy()
mi = Moran(y, w)

print(f"Moran's I: {mi.I:.4f}")
print(f"p-value (permutation): {mi.p_sim:.4f}")
print(f"z-score (normal approx): {mi.z_norm:.3f}")

# 6) Diagnostics: check for MSOAs without neighbors
no_neighbor = sum(len(nbrs) == 0 for nbrs in w.neighbors.values())
print(f"MSOAs with zero neighbors (should be 0): {no_neighbor}")

# 7) Visualization
fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# 7a) Choropleth: counts per MSOA
msoas_counted.plot(
    column="count",
    ax=axes[0],
    cmap="viridis",
    linewidth=0.4,
    edgecolor="black",
    legend=True
)
axes[0].set_title("Professional Host Listings by MSOA")
axes[0].axis("off")

# 7b) Moran scatter plot (standardized counts vs. spatial lag)
lag_y = np.array([np.sum(y[w.neighbors[i]]) / (len(w.neighbors[i]) or 1) for i in range(len(y))])
y_std = (y - y.mean()) / y.std()
lag_y_std = (lag_y - lag_y.mean()) / lag_y.std()

axes[1].scatter(y_std, lag_y_std, s=12, alpha=0.7, color="tab:blue")
axes[1].axhline(0, color="grey", lw=0.8)
axes[1].axvline(0, color="grey", lw=0.8)
axes[1].set_title("Moran's I Scatter (Professional Hosts per MSOA)")
axes[1].set_xlabel("Standardized counts")
axes[1].set_ylabel("Standardized spatial lag")

plt.tight_layout()
plt.show()
```

Moran’s I of 0.6465 with a p-value of 0.001 indicates strong, statistically significant spatial clustering of multi-host listings across London MSOAs. High-count areas are surrounded by other high-count areas, confirming that the distribution is far from random. This suggests concentrated zones of multi-host activity that may warrant targeted policy attention.

Having confirmed spatial autocorrelation with Moran’s I, I will now test the spatial lag and error models to assess whether the variation in multi-host density is better explained by neighboring effects or unobserved spatial structure in the census-like boundary data.

#### Spatial Lag Model
```{python}
from spreg import ML_Lag

# Dependent variable: counts
y = msoas_counted["count"].to_numpy()

# No predictors → just intercept
X = np.ones((len(y), 1))

# Ensure weights are row-standardized
w.transform = "r"

# Fit spatial lag model
lag_model = ML_Lag(y, X, w=w, name_y="count", name_x=["Intercept"])

# Print results
print("\nSpatial Lag Model (counts only)")
print(f"ρ (spatial lag coefficient): {lag_model.rho:.4f}")
print(f"Pseudo R²: {lag_model.pr2:.4f}")
print(f"AIC: {lag_model.aic:.2f}")
print(f"Intercept: {lag_model.betas[0][0]:.4f}")

# Attach predicted values and residuals for mapping
msoas_counted["predicted_lag"] = lag_model.predy
msoas_counted["residuals_lag"] = lag_model.u
```

```{python} 
import matplotlib.pyplot as plt

fig, axes = plt.subplots(1, 2, figsize=(16, 7))

# --- Map of predicted values from spatial lag model ---
msoas_counted.plot(
    column="predicted_lag",
    cmap="plasma",
    linewidth=0.4,
    edgecolor="black",
    legend=True,
    ax=axes[0]
)
axes[0].set_title("Predicted Professional Host Counts (Spatial Lag Model)")
axes[0].axis("off")

# --- Map of residuals from spatial lag model ---
msoas_counted.plot(
    column="residuals_lag",
    cmap="coolwarm",
    linewidth=0.4,
    edgecolor="black",
    legend=True,
    ax=axes[1]
)
axes[1].set_title("Residuals of Spatial Lag Model (Professional Hosts)")
axes[1].axis("off")

plt.tight_layout()
plt.show()
```

While I ran the spatial lag model to explore potential spillover effects between neighboring MSOAs, I don’t believe it adds meaningful insight in this context. Just because two areas are adjacent doesn’t imply that multi-host listing counts in one directly influence the other — these are not inherently relational phenomena. The clustering observed is more likely due to shared local conditions than spatial dependence, so including the lag model may misrepresent the underlying dynamics.

Since adjacency alone doesn’t imply causal influence between MSOAs, I turn instead to a spatial error model to capture unobserved spatial clustering in the residuals.

#### Spatial Error Model
```{python}
import numpy as np
from libpysal.weights import KNN
from spreg import GM_Error

# --- Merge professional host counts into MSOA shapefile ---
msoas_full = msoas.merge(
    msoas_counted[["MSOA11CD", "count"]],  # professional host counts
    on="MSOA11CD",
    how="left"
).fillna({"count": 0})

# --- Dependent variable: professional host counts ---
y = msoas_full["count"].values.reshape((-1, 1))

# --- Explanatory variables: socio-demographic attributes ---
X = msoas_full[["USUALRES", "POPDEN", "HHOLDS", "AVHHOLDSZ"]].values

# --- Spatial weights: KNN (avoids islands) ---
w = KNN.from_dataframe(msoas_full, k=8)
w.transform = "r"

# --- Spatial error model ---
model = GM_Error(
    y,
    X,
    w=w,
    name_y="prof_count",
    name_x=["USUALRES", "POPDEN", "HHOLDS", "AVHHOLDSZ"]
)

# --- Display results ---
print(model.summary)
```

The model shows that while household variables like number of households and average household size are positively associated with multi‑host listings, population and density are negatively associated. However, the most striking result is the very high and significant spatial error coefficient (lambda ≈ 0.93), which indicates that even after accounting for these demographic factors, there remains strong spatial autocorrelation in the residuals. In other words, clustering of multi‑host activity is not fully explained by population or housing characteristics alone, but is (obvioiusly) impacted by another variable. If we could find some data on tourism, housing cost, or anything else like this that could definitely further expand this.

However, crucially this above output is not very presentable to a mayor. So while we can maybe discuss it, saying our own research showed that these variables used show that there are other factors that need to be accounted for, I think it would be better not to show the output as it might not be visually pleasing (like even I don't wanna look at it realistically).

#### Getis-Ord Gi
Next, I will use the Getis‑Ord Gi statistic to identify local hotspots and coldspots of multi‑host activity.
```{python}
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from libpysal.weights import KNN
from esda.getisord import G_Local
import matplotlib.patches as mpatches

# ====== Settings ======
MSOA_ID_COL = "MSOA11CD"
K_NEIGHBORS = 5  # for KNN spatial weights

# ====== Spatial weights ======
w = KNN.from_dataframe(msoas_counted, k=K_NEIGHBORS, use_index=True)
w.transform = "r"

# ====== Run Gi* statistic ======
y = msoas_counted["count"].astype(float).values  # professional host counts
gi = G_Local(
    y,
    w,
    permutations=499,  # fewer permutations for speed
    n_jobs=1           # single-threaded
)

# ====== Add results to GeoDataFrame ======
msoas_counted["GiZ"] = gi.Zs
msoas_counted["GiP"] = gi.p_sim
msoas_counted["GiSig"] = np.where(
    (msoas_counted["GiP"] < 0.05) & (msoas_counted["GiZ"] > 0), "Hotspot",
    np.where((msoas_counted["GiP"] < 0.05) & (msoas_counted["GiZ"] < 0), "Coldspot", "Not Significant")
)

# ====== Quick preview ======
print(msoas_counted[[MSOA_ID_COL, "count", "GiZ", "GiP", "GiSig"]].head())

# ====== Plot hotspots ======
fig, ax = plt.subplots(figsize=(10, 10))

color_map = {
    "Hotspot": "#d7191c",         # Red
    "Coldspot": "#2c7bb6",        # Blue
    "Not Significant": "#cccccc"  # Grey
}

msoas_counted.plot(
    ax=ax,
    color=msoas_counted["GiSig"].map(color_map),
    edgecolor="black",
    linewidth=0.3
)

# ====== Legend ======
legend_patches = [
    mpatches.Patch(color=color_map["Hotspot"], label="Hotspot"),
    mpatches.Patch(color=color_map["Coldspot"], label="Coldspot"),
    mpatches.Patch(color=color_map["Not Significant"], label="Not Significant")
]
ax.legend(handles=legend_patches, title="Gi* Results", loc="lower left")

# ====== Final styling ======
ax.set_title("Getis–Ord Gi* Hotspots of Professional Host Properties", fontsize=14)
ax.axis("off")
plt.tight_layout()
plt.show()
```

So because the MSOA data for some reason doesn't recognise that the polygons are connected to one another (so it thinks they are seperate) I had to define the number of neighbours (which I set to 5 so it isn't too many and over the top of a strange pattern) but this does mean this map is not entirely accurate. It is still interesting though.

Now I will do the same with the boroughs, which doesn't have that problem of neighbouring. I think this could be good to show initially (so it is clear where the hotspots and relevant boroughs are as I think this is more important for policy than MSOAs)

```{python}
import geopandas as gpd
import numpy as np
import matplotlib.pyplot as plt
from libpysal.weights import Queen
from esda.getisord import G_Local
import matplotlib.patches as mpatches

# ====== Step 1: CRS alignment ======
gdf_multi_hosts = gdf_multi_hosts.to_crs(epsg=27700)
boroughs = boroughs.to_crs(epsg=27700)

# ====== Step 2: Spatial join to assign listings to boroughs ======
joined_b = gpd.sjoin(
    gdf_multi_hosts,
    boroughs[["NAME", "geometry"]],
    how="left",
    predicate="within"
)

# ====== Step 3: Count professional host listings per borough ======
borough_counts = joined_b.groupby("NAME").size().reset_index(name="raw_count")

# ====== Step 4: Merge counts back to borough polygons ======
boroughs_counted = boroughs.merge(borough_counts, on="NAME", how="left")
boroughs_counted["raw_count"] = boroughs_counted["raw_count"].fillna(0).astype(int)

# ====== Step 5: Normalize by borough area (per km²) ======
boroughs_counted["area_km2"] = boroughs_counted.geometry.area / 1e6
boroughs_counted["count_norm"] = boroughs_counted["raw_count"] / boroughs_counted["area_km2"]

# ====== Step 6: Build Queen contiguity spatial weights ======
w = Queen.from_dataframe(boroughs_counted)
w.transform = "r"

# ====== Step 7: Run Gi* statistic on normalized counts ======
y = boroughs_counted["count_norm"].astype(float).values
gi = G_Local(y, w, permutations=499, n_jobs=1)  # safe defaults

# ====== Step 8: Attach Gi* results ======
boroughs_counted["GiZ"] = gi.Zs
boroughs_counted["GiP"] = gi.p_sim
boroughs_counted["GiSig"] = np.where(
    (boroughs_counted["GiP"] < 0.05) & (boroughs_counted["GiZ"] > 0), "Hotspot",
    np.where((boroughs_counted["GiP"] < 0.05) & (boroughs_counted["GiZ"] < 0), "Coldspot", "Not Significant")
)

# ====== Step 9: Map with color legend ======
fig, ax = plt.subplots(figsize=(10, 10))
color_map = {
    "Hotspot": "#d7191c",
    "Coldspot": "#2c7bb6",
    "Not Significant": "#cccccc"
}

boroughs_counted.plot(
    ax=ax,
    color=boroughs_counted["GiSig"].map(color_map),
    edgecolor="black",
    linewidth=0.5
)

legend_patches = [mpatches.Patch(color=color_map[k], label=k) for k in color_map]
ax.legend(handles=legend_patches, title="Gi* Classification", loc="lower left")

ax.set_title("Getis–Ord Gi* Hotspots of Professional Host Properties (Normalized by Area)", fontsize=14)
ax.axis("off")
plt.tight_layout()
plt.show()
```

This map uses the standard Getis–Ord Gi* method with Queen contiguity and a 95% confidence threshold, which is widely accepted in spatial analysis. The input values are normalized by borough area (per km²), ensuring fair comparison across differently sized regions. While this conservative approach may underdetect coldspots, it ensures statistical rigor and avoids overinterpreting weak patterns. For exploratory purposes, a relaxed threshold (e.g. 90%) can be used in parallel to surface borderline cases.